# robots.txt for marcusgoll.com
# LLM SEO Optimization - Feature 052, US1, T006

# Default crawlers (allow all)
User-agent: *
Allow: /
Sitemap: https://marcusgoll.com/sitemap.xml

# AI Search Crawlers - Allow for citations and AI-powered search
# These crawlers power search features in AI assistants (not training)

# OpenAI ChatGPT Search - Allow for AI citations
User-agent: ChatGPT-User
Allow: /

# Anthropic Claude Search - Allow for AI citations
User-agent: Claude-Web
Allow: /

# Perplexity AI Search - Allow for AI citations
User-agent: PerplexityBot
Allow: /

# AI Training Crawlers - Block for content protection
# These crawlers are used to train large language models

# OpenAI GPTBot - Block training
User-agent: GPTBot
Disallow: /

# Anthropic ClaudeBot - Block training
User-agent: ClaudeBot
Disallow: /

# Google Extended - Block Gemini/Bard training
User-agent: Google-Extended
Disallow: /

# Common Crawl - Block general web scraping for training
User-agent: CCBot
Disallow: /

# Bytedance/TikTok - Block training
User-agent: Bytespider
Disallow: /

# Meta AI - Block training
User-agent: FacebookBot
Disallow: /

# Crawl delay for respectful crawling
Crawl-delay: 1
